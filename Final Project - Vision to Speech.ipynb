{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb432fe-fcff-426c-8076-944fc2e4259d",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99360467-5c56-46f3-851e-ebc5d68b36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75227d4d-8b47-4a16-96da-418f3d53cf94",
   "metadata": {},
   "source": [
    "### Set up the image captioning environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24f73b5-d295-4694-8389-6179579c3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize the processor and model fro image captioning\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# pre-trained BLIP model from Salesforce for generating captions from images\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Initialize the semantic similarity model (pre-trained SentenceTransformer model)to measure the similarity between sentences\n",
    "semantic_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec2933-23a3-4d44-aaad-01d110425647",
   "metadata": {},
   "source": [
    "### Create caption for frame   \n",
    "##### Convert a single frame to text using the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54cdb9dc-5323-4c3e-81b4-05758bfff989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_text(frame):\n",
    "    # Convert the frame (an array) to an image object\n",
    "    image = Image.fromarray(frame)\n",
    "    \n",
    "    # Process the image to prepare it for the model, returning tensors in PyTorch format\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Use the model to generate a text description for the image, allowing up to 50 new tokens\n",
    "    out = model.generate(**inputs, max_new_tokens=50)\n",
    "    \n",
    "    # Decode the generated output to a readable string, skipping special tokens\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cc2e0c-a588-47b8-a221-3fe3ea4a0d4c",
   "metadata": {},
   "source": [
    "###  Filter out similar text descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00799288-bf89-42f6-96eb-b3b9a84f2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_meanings(texts, threshold=0.75):\n",
    "    \n",
    "    # List to store unique texts\n",
    "    unique_texts = []  \n",
    "    \n",
    "    for text in texts:\n",
    "        # Convert the current text to a numerical format (embedding)\n",
    "        text_embedding = semantic_model.encode(text, convert_to_tensor=True)\n",
    "        #check if the text is unique\n",
    "        is_unique = True  \n",
    "        \n",
    "        for unique_text in unique_texts:\n",
    "            # Convert the existing unique text to a numerical format (embedding)\n",
    "            unique_text_embedding = semantic_model.encode(unique_text, convert_to_tensor=True)\n",
    "            # Calculate the similarity between the current text and the unique text\n",
    "            similarity = util.pytorch_cos_sim(text_embedding, unique_text_embedding).item()\n",
    "            # If similarity is above the threshold, mark as not unique\n",
    "            \n",
    "            if similarity > threshold:\n",
    "                is_unique = False\n",
    "                break\n",
    "        # If the text is unique, add it to the list of unique texts\n",
    "        if is_unique:\n",
    "            unique_texts.append(text)\n",
    "    \n",
    "    # Return the list of unique texts\n",
    "    return unique_texts  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6689e-a71e-412b-ba82-42c28acc9003",
   "metadata": {},
   "source": [
    "### Processes a video to generate meaningful text descriptions based on repeated actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a323544d-f2f4-4bdd-90de-9eb0d403a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_text(video_path, repeat_threshold=5):\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Variable to store the current action detected\n",
    "    current_action = None\n",
    "    # count how many times the current action is repeated\n",
    "    action_count = 0\n",
    "    # List to store text descriptions of frames\n",
    "    frame_texts = []\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Generate a text description for the frame\n",
    "        text = frame_to_text(frame)\n",
    "\n",
    "        # Check if the text is the same as the current action\n",
    "        if text == current_action:\n",
    "            action_count += 1\n",
    "        else:\n",
    "            # If the action count meets or exceeds the threshold, add it to frame_texts\n",
    "            if action_count >= repeat_threshold and current_action is not None:\n",
    "                frame_texts.append(f\"'{current_action}'.\")\n",
    "            current_action = text\n",
    "            action_count = 1\n",
    "\n",
    "    # Check the last action after exiting the loop\n",
    "    if action_count >= repeat_threshold and current_action is not None:\n",
    "        frame_texts.append(f\"'{current_action}'.\")\n",
    "        \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Filter out similar text descriptions to get unique meanings\n",
    "    unique_frame_texts = get_unique_meanings(frame_texts)\n",
    "\n",
    "    # Join the unique descriptions into a single string\n",
    "    video_description = \" \".join(unique_frame_texts)\n",
    "\n",
    "    # Return the final video description\n",
    "    return video_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3effb94-ba70-40b7-99d7-65e3fe7cbb20",
   "metadata": {},
   "source": [
    "###  Translating text from English to Hausa and convert that translated text into speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0265587d-9233-4e53-bae8-eadf5fba3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang='en'):\n",
    "    # Create a gTTS (Google Text-to-Speech) object with the text and language\n",
    "    tts = gTTS(text=text, lang=lang)  \n",
    "    # Save the generated speech to an MP3 file named 'output.mp3'\n",
    "    tts.save(\"output.mp3\")  \n",
    "    # Play the audio file using the system's default audio player\n",
    "    os.system(\"afplay output.mp3\")\n",
    "\n",
    "def translate_text(text, target_lang='ha'):\n",
    "    # Create a Translator object from the googletrans library\n",
    "    translator = Translator()  \n",
    "    # Translate the text to the target language\n",
    "    translation = translator.translate(text, dest=target_lang)\n",
    "    # Return the translated text\n",
    "    return translation.text  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de50b53-323d-4e20-a389-f96798ca1a8c",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11293f94-b528-47fb-8a2f-37d912e39b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Description: 'a group of men playing soccer on a field'. 'a soccer player is kicking the ball'. 'a soccer player is trying to block the ball'.\n",
      "\n",
      "Translated Description: 'rukunin maza suna wasa ƙwallon ƙafa a filin'.'Dan wasan ƙwallon ƙafa ya koma kwallon'.'Dan wasan ƙwallon ƙafa yana ƙoƙarin toshe kwallon..\n"
     ]
    }
   ],
   "source": [
    "video_path = \"/Users/ronny/Downloads/Test_Videos/Football.mp4\"\n",
    "\n",
    "description = video_to_text(video_path)\n",
    "print(\"Original Description:\", description)\n",
    "\n",
    "print()\n",
    "\n",
    "# Translate the description to Hausa\n",
    "translated_description = translate_text(description, target_lang='ha')\n",
    "print(\"Translated Description:\", translated_description)\n",
    "\n",
    "# Convert the translated text to speech\n",
    "text_to_speech(translated_description, lang='ha')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
